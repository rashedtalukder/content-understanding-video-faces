{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Face-Aware Content Understanding for Video \n",
    "## Objective\n",
    "This document is meant to present a guideline on how to leverage the Azure AI Content Understanding API for your video content with face features enabled.\n",
    "\n",
    "## Pre-requisites\n",
    "1. [Azure Account and Subscription](https://azure.microsoft.com/en-us/free/)\n",
    "2. [Azure AI Resource](https://review.learn.microsoft.com/en-us/azure/ai-studio/how-to/create-azure-ai-resource?branch=main)\n",
    "3. [Azure AI Resource Face Gating](https://learn.microsoft.com/en-us/legal/cognitive-services/computer-vision/limited-access-identity?context=%2Fazure%2Fai-services%2Fcomputer-vision%2Fcontext%2Fcontext#registration-process) Select `[Video Indexer] Facial Identification (1:N or 1:1 matching) to search for a face in a media or entertainment video archive to find a face within a video and generate metadata for media or entertainment use cases only` in the registration form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!! Restart the kernel if any package was installed or updated in the installation step !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set configurations and review checklist\n",
    "\n",
    "All configurations are expected to be set in the file `./.env`. If the file does not exist, it will be created. You will need to open the file and check/set all configuration variables.\n",
    "\n",
    "Verify each item in the checklist below while you are working on the file `./.env`.\n",
    "1. Make sure you have an Azure account.\n",
    "1. Make sure you have an Azure AI Services resource that will be used to access Azure Content Understanding\n",
    "   1. Obtain the endpoint of this resource and assign it to `AZURE_AI_SERVICES_ENDPOINT`.\n",
    "   1. The pre-filled value for `AZURE_AI_SERVICES_API_VERSION` is good as of August 2024.\n",
    "   1. For authentication to this resource\n",
    "      1. If key-based authentication is used, obtain the API key and assign it to `AZURE_AI_SERVICES_API_KEY`.\n",
    "      1. Otherwise, Entra ID/AAD authentication is used. You will need to\n",
    "         1. Leave the value for `AZURE_AI_SERVICES_API_KEY` empty.\n",
    "         2. Open a terminal and run `az login --use-device-code` to login to Azure\n",
    "         3. Assign `Cognitive Service User` role to yourself for this resource\n",
    "\n",
    "1. For `VIDEO_LOCATION`, it could be a video URL or a local file.\n",
    "   1. If you put a video url, make sure your video is publicly accessible.\n",
    "   1. A local file with the absolute path. If you use this option, the video length must be less than 30 minutes and file size must be less than 500MB\n",
    "\n",
    "If each item in the checklist is checked, save the file `./.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    print(\".env file already exists. Assuming all configurations are set.\")\n",
    "else:\n",
    "    print(\".env file does not exist. Creating one.\")\n",
    "    print(\"Please open the file and check/set all configurations.\")\n",
    "    print(\"After that, you may proceed to the next cell.\")\n",
    "    !cp .env.example .env\n",
    "\n",
    "    assert False, \"Stop the notebook execution on purpose to allow user configuration.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "VIDEO_LOCATION = os.getenv(\"VIDEO_LOCATION\", \"./inputs/{{VIDEO_FILENAME}}\")\n",
    "AZURE_AI_SERVICES_ENDPOINT = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\", \"\")\n",
    "AZURE_AI_SERVICES_API_VERSION = os.getenv(\"AZURE_AI_SERVICES_API_VERSION\", \"\")\n",
    "AZURE_AI_SERVICES_API_KEY = os.getenv(\"AZURE_AI_SERVICES_API_KEY\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert VIDEO_LOCATION, \"VIDEO_LOCATION is not set\"\n",
    "assert AZURE_AI_SERVICES_ENDPOINT, \"AZURE AI SERVICES ENDPOINT is not set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom analyzer with pre-defined schema for videos\n",
    "The custom analyzer schema is defined in [./templates/video_face_aware.json](./templates/video_face_aware.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parents[0]\n",
    "\n",
    "sys.path.append(str(parent_dir))  # add\n",
    "print(str(parent_dir))\n",
    "from utility.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "ANALYZER_TEMPLATE_PATH = \"./templates/video_face_aware.json\"\n",
    "ANALYZER_ID = \"video_face_analyzer\" + \"_\" + str(\n",
    "    uuid.uuid4())  # Unique identifier for the analyzer\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICES_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICES_API_VERSION,\n",
    "    subscription_key=AZURE_AI_SERVICES_API_KEY,\n",
    "    enable_face_identification=True)\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(\n",
    "    ANALYZER_ID, analyzer_schema_path=ANALYZER_TEMPLATE_PATH)\n",
    "\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Write Results Out to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results_file = \"./outputs/video_cu_results.json\"\n",
    "with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(video_cu_result, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Get and Save Key Frames and Face Thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "\n",
    "def save_image(image_id: str):\n",
    "    raw_image = cu_client.get_image_from_analyze_operation(analyze_response=response,\n",
    "        image_id=image_id\n",
    "    )\n",
    "    image = Image.open(BytesIO(raw_image))\n",
    "    # image.show()\n",
    "    image.save(f\"{image_id}.jpg\", \"JPEG\")\n",
    "\n",
    "\n",
    "# Initialize sets for unique face IDs and keyframe IDs\n",
    "face_ids = set()\n",
    "keyframe_ids = set()\n",
    "\n",
    "# Extract unique face IDs safely\n",
    "result_data = video_cu_result.get(\"result\", {})\n",
    "contents = result_data.get(\"contents\", [])\n",
    "\n",
    "# Iterate over contents to find faces and keyframes if available\n",
    "for content in contents:\n",
    "    # Safely retrieve face IDs if \"faces\" exists and is a list\n",
    "    faces = content.get(\"faces\", [])\n",
    "    if isinstance(faces, list):\n",
    "        for face in faces:\n",
    "            face_id = face.get(\"faceId\")\n",
    "            if face_id:\n",
    "                face_ids.add(f\"face.{face_id}\")\n",
    "\n",
    "    # Extract keyframe IDs from \"markdown\" if it exists and is a string\n",
    "    markdown_content = content.get(\"markdown\", \"\")\n",
    "    if isinstance(markdown_content, str):\n",
    "        keyframe_ids.update(re.findall(r\"(keyFrame\\.\\d+)\\.jpg\", markdown_content))\n",
    "\n",
    "# Output the results\n",
    "print(\"Unique Face IDs:\", face_ids)\n",
    "print(\"Unique Keyframe IDs:\", keyframe_ids)\n",
    "\n",
    "# Save all face images\n",
    "for face_id in face_ids:\n",
    "    save_image(face_id)\n",
    "\n",
    "# Save all keyframe images\n",
    "for keyframe_id in keyframe_ids:\n",
    "    save_image(keyframe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete exist analyzer in Azure Understanding Content Service\n",
    "This snippet is not required, but it's only used to prevent the testing analyzer from residing in your service. The custom fields analyzer could be stored in your service for reusing by subsequent business in real usage scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
